{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import pydot\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from pprint import pprint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "from utils import vgg as nn\n",
    "from utils import roi_helpers\n",
    "from utils import losses as losses_fn\n",
    "from utils.simple_parser import get_data\n",
    "from utils import config, data_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 64\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigProto = tf.compat.v1.ConfigProto()\n",
    "ConfigProto.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=ConfigProto)\n",
    "# K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config has been written to config.pickle, and can be loaded when testing to ensure correct results\n"
     ]
    }
   ],
   "source": [
    "cfg = config.Config()\n",
    "\n",
    "cfg.epoch_length = 5 # iteration 개수 (train: 1409, val: 456)\n",
    "cfg.num_epochs = 2 # 총 Epochs\n",
    "# cfg.rot_90 = True # default\n",
    "# cfg.use_random_brightness = True # add\n",
    "# cfg.use_vertical_flips = True # default\n",
    "# cfg.use_horizontal_flips = True # default\n",
    "cfg.num_rois = 128 # default\n",
    "cfg.std_scaling = 4 # default\n",
    "# cfg.base_net_weights = os.path.join('./model/', nn.get_weight_path())\n",
    "\n",
    "# TODO: the only file should to be change for other data to train\n",
    "cfg.model_path = './model/fusion_improved_last.hdf5'\n",
    "cfg.visual_model_path = './model/fusion_improved.png'\n",
    "cfg.train_label_file = 'data/train_labels.txt'\n",
    "cfg.val_label_file = 'data/val_labels.txt'\n",
    "cfg.class_mapping = {\n",
    "    'small-vehicle': 0,\n",
    "    'large-vehicle': 1,\n",
    "    'harbor': 2,\n",
    "    'ship': 3,\n",
    "    'ground-track-field': 4,\n",
    "    'soccer-ball-field': 5,\n",
    "    'baseball-diamond': 6,\n",
    "    'swimming-pool': 7,\n",
    "    'roundabout': 8,\n",
    "    'tennis-court': 9,\n",
    "    'basketball-court': 10,\n",
    "    'plane': 11,\n",
    "    'helicopter': 12,\n",
    "    'bridge': 13,\n",
    "    'storage-tank': 14,\n",
    "    'bg': 15\n",
    "}\n",
    "cfg.len_class = len(cfg.class_mapping)\n",
    "\n",
    "with open(cfg.config_save_file, 'wb') as config_f:\n",
    "    pickle.dump(cfg, config_f)\n",
    "    print('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(cfg.config_save_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load\n",
    "- train: 98990it (03:09)\n",
    "- valid: 28853it (01:02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images, train_classes_count = get_data(cfg.train_label_file, cfg.class_mapping)\n",
    "# validation_images, validation_classes_count = get_data(cfg.val_label_file, cfg.class_mapping)\n",
    "\n",
    "# with open('data/train_images.json', 'w', encoding='utf-8') as file:\n",
    "#     json.dump(train_images, file, indent='\\t')\n",
    "    \n",
    "# with open('data/train_classes_count.json', 'w', encoding='utf-8') as file:\n",
    "#     json.dump(train_classes_count, file, indent='\\t')\n",
    "    \n",
    "# with open('data/validation_images.json', 'w', encoding='utf-8') as file:\n",
    "#     json.dump(validation_images, file, indent='\\t')\n",
    "    \n",
    "# with open('data/validation_classes_count.json', 'w', encoding='utf-8') as file:\n",
    "#     json.dump(validation_classes_count, file, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_images.json', 'r', encoding='utf-8') as file:\n",
    "    train_images = json.load(file)\n",
    "    \n",
    "with open('data/train_classes_count.json', 'r', encoding='utf-8') as file:\n",
    "    train_classes_count = json.load(file)\n",
    "    \n",
    "with open('data/validation_images.json', 'r', encoding='utf-8') as file:\n",
    "    validation_images = json.load(file)\n",
    "    \n",
    "with open('data/validation_classes_count.json', 'r', encoding='utf-8') as file:\n",
    "    validation_classes_count = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes (including bg) = 16\n",
      "==Training Num samples 1409 , images per class:\n",
      "{'baseball-diamond': 415,\n",
      " 'basketball-court': 515,\n",
      " 'bg': 0,\n",
      " 'bridge': 2047,\n",
      " 'ground-track-field': 325,\n",
      " 'harbor': 5983,\n",
      " 'helicopter': 630,\n",
      " 'large-vehicle': 16969,\n",
      " 'plane': 8055,\n",
      " 'roundabout': 399,\n",
      " 'ship': 28068,\n",
      " 'small-vehicle': 26126,\n",
      " 'soccer-ball-field': 326,\n",
      " 'storage-tank': 5029,\n",
      " 'swimming-pool': 1736,\n",
      " 'tennis-court': 2367}\n",
      "==Validation Num samples 456 , images per class:\n",
      "{'baseball-diamond': 214,\n",
      " 'basketball-court': 132,\n",
      " 'bg': 0,\n",
      " 'bridge': 464,\n",
      " 'ground-track-field': 144,\n",
      " 'harbor': 2090,\n",
      " 'helicopter': 73,\n",
      " 'large-vehicle': 4387,\n",
      " 'plane': 2531,\n",
      " 'roundabout': 179,\n",
      " 'ship': 8960,\n",
      " 'small-vehicle': 5438,\n",
      " 'soccer-ball-field': 153,\n",
      " 'storage-tank': 2888,\n",
      " 'swimming-pool': 440,\n",
      " 'tennis-court': 760}\n"
     ]
    }
   ],
   "source": [
    "print('Num classes (including bg) = {}'.format(cfg.len_class))\n",
    "\n",
    "print('==Training Num samples {} , images per class:'.format(len(train_images)))\n",
    "pprint(train_classes_count)\n",
    "\n",
    "print('==Validation Num samples {} , images per class:'.format(len(validation_images)))\n",
    "pprint(validation_classes_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trainGenerator():\n",
    "#     for x, y in zip(range(10), range(10,20)):\n",
    "#         yield (x, y)\n",
    "\n",
    "# train_dataset  = tf.data.Dataset.from_generator(\n",
    "#     trainGenerator, \\\n",
    "#     (tf.int32, tf.float32), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    data_gen_train = data_generators.get_anchor_gt(train_images, train_classes_count, cfg, nn.get_img_output_length, mode='train')\n",
    "    data_gen_val = data_generators.get_anchor_gt(validation_images, validation_classes_count, cfg, nn.get_img_output_length, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, (Ycls, Yregr), aug = next(data_gen_train)\n",
    "# print(X.shape, Ycls.shape, Yregr.shape)\n",
    "# print(Ycls.sum(), Yregr.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    input_shape_img = (None, None, 3)\n",
    "\n",
    "    img_input = Input(shape=input_shape_img, name='img_input')\n",
    "    roi_input = Input(shape=(None, 4), name='roi_input') # 4 == 점의 개수 * 2\n",
    "\n",
    "    # define the base network (resnet here, can be VGG, Inception, etc)\n",
    "    shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "    # define the RPN, built on the base layers\n",
    "    num_anchors = len(cfg.anchor_box_scales) * len(cfg.anchor_box_ratios) # 3 * 3 == 9\n",
    "    rpn = nn.rpn(shared_layers, num_anchors) # rpn_out_class 2k이어야 하는데 k 반환\n",
    "    \n",
    "    # cfg.num_rois : 32\n",
    "    classifier = nn.classifier(shared_layers, roi_input, cfg.num_rois, nb_classes=cfg.len_class, trainable=True)\n",
    "\n",
    "    model_rpn = Model(img_input, rpn[:2], name='model_rpn')\n",
    "    model_classifier = Model([img_input, roi_input], classifier, name='model_classifier')\n",
    "\n",
    "    # this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
    "    model_all = Model([img_input, roi_input], rpn[:2] + classifier, name='model_all')\n",
    "\n",
    "    model_rpn.compile(optimizer=Adam(lr=1e-5), \n",
    "                      loss=[losses_fn.rpn_loss_cls(num_anchors), losses_fn.rpn_loss_regr(num_anchors)])\n",
    "    \n",
    "    model_classifier.compile(optimizer=Adam(lr=1e-5),\n",
    "                             loss=[losses_fn.class_loss_cls, losses_fn.class_loss_regr(cfg.len_class - 1)],\n",
    "                             metrics={'dense_class_{}'.format(cfg.len_class): 'accuracy'})\n",
    "    \n",
    "    model_all.compile(optimizer='sgd', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model_all, to_file=cfg.visual_model_path, show_shapes=True, \\\n",
    "#            show_layer_names=True, expand_nested=False, dpi=200)\n",
    "\n",
    "# display(Image.open(cfg.visual_model_path))\n",
    "# model_all.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_length = 5 # len(train_images) # int(cfg.epoch_length)\n",
    "num_epochs = int(cfg.num_epochs)\n",
    "iter_num = 0\n",
    "\n",
    "losses = np.zeros((epoch_length, 5))\n",
    "best_loss = np.Inf\n",
    "\n",
    "rpn_accuracy_for_epoch = []\n",
    "\n",
    "class_mapping_inv = {v: k for k, v in cfg.class_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "P2433.png (1956, 1930, 3)\n",
      "P2433.png (1968, 1936, 3)\n",
      "success data_gen_train (1, 246, 242, 18) (1, 246, 242, 72)\n",
      "y_rpn_overlap: Tensor(\"rpn_loss_regr_fixed_num/strided_slice_2:0\", shape=(535788,), dtype=float32)\n",
      "L_IIOU: Tensor(\"rpn_loss_regr_fixed_num/sub_12:0\", shape=(535788,), dtype=float32)\n",
      "mean: Tensor(\"rpn_loss_regr_fixed_num/Mean:0\", shape=(), dtype=float32)\n",
      "y_rpn_overlap: Tensor(\"rpn_loss_regr_fixed_num/strided_slice_2:0\", shape=(535788,), dtype=float32)\n",
      "L_IIOU: Tensor(\"rpn_loss_regr_fixed_num/sub_12:0\", shape=(535788,), dtype=float32)\n",
      "mean: Tensor(\"rpn_loss_regr_fixed_num/Mean:0\", shape=(), dtype=float32)\n",
      "loss\n",
      "-136739952.0 7.6683526039123535\n",
      "X2 is None None\n",
      "P2379.png (2251, 2085, 3)\n",
      "P2379.png (2256, 2096, 3)\n",
      "success data_gen_train (1, 282, 262, 18) (1, 282, 262, 72)\n",
      "y_rpn_overlap: Tensor(\"rpn_loss_regr_fixed_num/strided_slice_2:0\", shape=(664956,), dtype=float32)\n",
      "L_IIOU: Tensor(\"rpn_loss_regr_fixed_num/sub_12:0\", shape=(664956,), dtype=float32)\n",
      "mean: Tensor(\"rpn_loss_regr_fixed_num/Mean:0\", shape=(), dtype=float32)\n",
      "loss\n",
      "6249988.0 8.01143741607666\n",
      "X2 is not None:  (1, 5, 4)\n",
      "class_loss_cls (1, 128, 16) , (1, 128, 16)\n",
      "class_loss_cls <dtype: 'int32'> , <dtype: 'float32'>\n",
      "class_loss_regr (1, 128, 120) , (1, 128, 60)\n",
      "class_loss_regr <dtype: 'int32'> , <dtype: 'float32'>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "in user code:\n\n    c:\\users\\ipsl\\.conda\\envs\\dota\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\ipsl\\Desktop\\Faster-RCNN-for-Dota\\utils\\losses.py:79 class_loss_regr_fixed_num  *\n        assert(y_true.shape == y_pred.shape)\n\n    AssertionError: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ff1877259f4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0my_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mloss_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mY1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;31m####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ipsl\\.conda\\envs\\dota\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1725\u001b[0m                                                     class_weight)\n\u001b[0;32m   1726\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1727\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ipsl\\.conda\\envs\\dota\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ipsl\\.conda\\envs\\dota\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ipsl\\.conda\\envs\\dota\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 725\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    726\u001b[0m             *args, **kwds))\n\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ipsl\\.conda\\envs\\dota\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ipsl\\.conda\\envs\\dota\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ipsl\\.conda\\envs\\dota\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ipsl\\.conda\\envs\\dota\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ipsl\\.conda\\envs\\dota\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ipsl\\.conda\\envs\\dota\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: in user code:\n\n    c:\\users\\ipsl\\.conda\\envs\\dota\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\ipsl\\Desktop\\Faster-RCNN-for-Dota\\utils\\losses.py:79 class_loss_regr_fixed_num  *\n        assert(y_true.shape == y_pred.shape)\n\n    AssertionError: \n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch_num in range(num_epochs):\n",
    "        progbar = Progbar(epoch_length)\n",
    "        print('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n",
    "        \n",
    "        for iter_num in range(epoch_length):\n",
    "            \n",
    "            X, Y, img_data, cache = next(data_gen_train)\n",
    "            print(\"success data_gen_train\", Y[0].shape, Y[1].shape)\n",
    "            \n",
    "\n",
    "            loss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "            print(\"loss\")\n",
    "            print(loss_rpn[0], loss_rpn[1])\n",
    "\n",
    "            P_rpn = model_rpn.predict_on_batch(X) # (rpn cls, rpn regr)\n",
    "\n",
    "            result = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], cfg, 'tf', use_regr=True,\n",
    "                                            overlap_thresh=0.7,\n",
    "                                            max_boxes=256) # origin code is 300 but paper is set by 256 (128 - positive, 128 - negative)\n",
    "            \n",
    "\n",
    "            # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "            X2, Y1, Y2, IouS = roi_helpers.calc_iou(result, img_data, cfg, cfg.class_mapping)\n",
    "\n",
    "            if X2 is None:\n",
    "                print(\"X2 is None\", X2)\n",
    "                rpn_accuracy_for_epoch.append(0)\n",
    "                continue\n",
    "            else:\n",
    "                print(\"X2 is not None: \", X2.shape)\n",
    "\n",
    "            neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "            pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "            rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "            \n",
    "            if len(neg_samples) > 0:\n",
    "                neg_samples = neg_samples[0]\n",
    "            else:\n",
    "                neg_samples = []\n",
    "\n",
    "            if len(pos_samples) > 0:\n",
    "                pos_samples = pos_samples[0]\n",
    "            else:\n",
    "                pos_samples = []\n",
    "            \n",
    "            if len(pos_samples) < cfg.num_rois // 2:\n",
    "                selected_pos_samples = pos_samples.tolist()\n",
    "            else:\n",
    "                selected_pos_samples = np.random.choice(pos_samples, cfg.num_rois // 2, replace=False).tolist()\n",
    "\n",
    "            if len(neg_samples) + len(selected_pos_samples) > cfg.num_rois:\n",
    "                selected_neg_samples = np.random.choice(neg_samples, cfg.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "            else:\n",
    "                selected_neg_samples = np.random.choice(neg_samples, cfg.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "\n",
    "            selected_samples = selected_pos_samples + selected_neg_samples\n",
    "\n",
    "            y_sample = model_classifier.predict([X, X2[:, selected_samples, :]])\n",
    "            loss_class = model_classifier.train_on_batch([X, X2[:, selected_samples, :]], [Y1[:, selected_samples, :], Y2[:, selected_samples, :]])\n",
    "            \n",
    "            ####\n",
    "            \n",
    "            print(\"img input:\", X.shape)\n",
    "            print(\"roi input:\", X2[:, selected_samples, :].shape, len(selected_samples))\n",
    "            print(X.shape, X2.shape, Y1.shape, Y2.shape, X.dtype, X2.dtype, Y1.dtype, Y2.dtype)\n",
    "\n",
    "            losses[iter_num, 0] = loss_rpn[1]\n",
    "            losses[iter_num, 1] = loss_rpn[2]\n",
    "\n",
    "            losses[iter_num, 2] = loss_class[1]\n",
    "            losses[iter_num, 3] = loss_class[2]\n",
    "            losses[iter_num, 4] = loss_class[3]\n",
    "\n",
    "            progbar.update(iter_num,\n",
    "                           [('rpn_cls', np.mean(losses[:iter_num, 0])), \\\n",
    "                            ('rpn_regr', np.mean(losses[:iter_num, 1])), \\\n",
    "                            ('detector_cls', np.mean(losses[:iter_num, 2])), \\\n",
    "                            ('detector_regr', np.mean(losses[:iter_num, 3]))])\n",
    "\n",
    "        loss_rpn_cls = np.mean(losses[:, 0])\n",
    "        loss_rpn_regr = np.mean(losses[:, 1])\n",
    "        loss_class_cls = np.mean(losses[:, 2])\n",
    "        loss_class_regr = np.mean(losses[:, 3])\n",
    "        class_acc = np.mean(losses[:, 4])\n",
    "\n",
    "        mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "\n",
    "        if cfg.verbose:\n",
    "            print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "            if mean_overlapping_bboxes == 0:\n",
    "                print('RPN is not producing bounding boxes that overlap the ground truth boxes.')\n",
    "                print('Check RPN settings or keep training.')\n",
    "                \n",
    "            print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "            print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "            print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "            print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "            print('Loss Detector regression: {}'.format(loss_class_regr))\n",
    "            print('Elapsed time: {}'.format(time.time() - start_time))\n",
    "\n",
    "        curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "\n",
    "        if curr_loss < best_loss:\n",
    "            if cfg.verbose:\n",
    "                print('Total loss decreasetd from {} to {}, saving weights'.format(best_loss, curr_loss))\n",
    "            best_loss = curr_loss\n",
    "            model_all.save_weights(cfg.model_path)\n",
    "\n",
    "        rpn_accuracy_for_epoch = []\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1[:, selected_samples, :].shape, Y2[:, selected_samples, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/2  \n",
    "P2379.png (2251, 2085, 3)\n",
    "P2379.png (2272, 2112, 3)\n",
    "success data_gen_train (1, 284, 264, 18) (1, 284, 264, 72)\n",
    "loss_cls (1, 284, 264, 18)\n",
    "loss_regr (1, 284, 264, 72)\n",
    "loss_cls (1, 284, 264, 18)\n",
    "loss_regr (1, 284, 264, 72)\n",
    "rpn train and predict (1, 284, 264, 9) (1, 284, 264, 36)\n",
    "img input: (1, 2272, 2112, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1930.png (2462, 1300, 3)\n",
    "rpn ground truth 419.92185068130493\n",
    "calc_rpn Error\n",
    "#calc_rpn Exception : can't multiply sequence by non-int of type 'float'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch 1/2\n",
    "5/5 [==============================] - 3270s 654s/step - rpn_cls: 6.2361 - rpn_regr: 0.7347 - detector_cls: 2.7627 - detector_regr: 0.2693\n",
    "Mean number of bounding boxes from RPN overlapping ground truth boxes: 1.9166666666666667\n",
    "- Classifier accuracy for bounding boxes from RPN: 0.7\n",
    "- Loss RPN classifier: 5.417903423309326\n",
    "- Loss RPN regression: 0.764133733510971\n",
    "- Loss Detector classifier: 2.743869400024414\n",
    "- Loss Detector regression: 0.1439718186855316\n",
    "- Elapsed time: 3269.942060947418\n",
    "- Total loss decreased from inf to 9.069878375530244, saving weights\n",
    "\n",
    "\n",
    "# Epoch 2/2\n",
    "5/5 [==============================] - 1960s 392s/step - rpn_cls: 5.6016 - rpn_regr: 0.7821 - detector_cls: 2.6116 - detector_regr: 0.3472\n",
    "Mean number of bounding boxes from RPN overlapping ground truth boxes: 1.1666666666666667\n",
    "\n",
    "- Classifier accuracy for bounding boxes from RPN: 0.9125\n",
    "- Loss RPN classifier: 4.953123617172241\n",
    "- Loss RPN regression: 0.8024014845490456\n",
    "- Loss Detector classifier: 2.5067588329315185\n",
    "- Loss Detector regression: 0.22378002405166625\n",
    "- Elapsed time: 1962.5114130973816\n",
    "- Total loss decreased from 9.069878375530244 to 8.486063958704472, saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, aug, cache = next(data_gen_train)\n",
    "y_rpn_cls, y_rpn_regr = Y\n",
    "y_rpn_overlap, y_rpn_regr = cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base = os.path.join('..' , 'data', 'train', 'images')\n",
    "colors = []\n",
    "grays = []\n",
    "\n",
    "for file in tqdm(os.listdir(base)):\n",
    "    img = tf.io.read_file(os.path.join(base, file))\n",
    "    img = tf.image.decode_image(img)\n",
    "    \n",
    "    if img.shape[-1] == 3:\n",
    "        colors.append(img.shape)\n",
    "    elif img.shape[-1] == 1:\n",
    "        grays.append(img.shape)\n",
    "    else:\n",
    "        print(\"ERROR : {}\".format(image.shape))\n",
    "    \n",
    "#     print(\"{} \\t {}\".format(file, img.shape))\n",
    "    \n",
    "print(\"colors : {} , grays: {}\".format(len(colors), len(grays)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = os.path.join('..' , 'data', 'train', 'images')\n",
    "img = tf.io.read_file(os.path.join(base, 'P1200.png'))\n",
    "img = tf.image.decode_image(img)\n",
    "print(img.shape)\n",
    "\n",
    "img = tf.image.grayscale_to_rgb(img)\n",
    "print(img.shape)\n",
    "\n",
    "# cv2.imwrite('img.jpg', img.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.pad(img, ((0, 0), (0, 16), (0, 0)), \"CONSTANT\", constant_values=0)\n",
    "img = tf.expand_dims(img, axis=0)\n",
    "img = tf.cast(img, dtype=tf.float32)\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_cls, rpn_regr = model_rpn.predict(img)\n",
    "print(rpn_cls.shape, rpn_regr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_cls_args = np.argwhere(rpn_cls >= 0.6)\n",
    "print(rpn_cls_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for args in rpn_cls_args:\n",
    "    print(rpn_regr[0, args[1], args[2], args[3]*4: args[3]*4 + 4])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rpn_regr, rpn_cls, class_cls = model_all.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.io.read_file(os.path.join('..' , 'data', 'train', 'images', 'P1311.png'))\n",
    "img = tf.image.decode_image(img)\n",
    "\n",
    "if len(img.shape) == 2 or img.shape[-1] == 1:\n",
    "    img = tf.image.grayscale_to_rgb(img)\n",
    "\n",
    "if img.shape[0] % 2 or img.shape[1] % 2:\n",
    "    pad_w, pad_h = 16 - img.shape[0] % 16, 16 - img.shape[1] % 16\n",
    "\n",
    "    if pad_h == 16:\n",
    "        pad_h = 0\n",
    "    if pad_w == 16:\n",
    "        pad_w = 0\n",
    "\n",
    "    paddings = tf.constant([[0, pad_h], [0, pad_w]])\n",
    "\n",
    "    img = tf.pad(img, paddings, \"CONSTANT\", constant_values=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
