{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import pydot\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from pprint import pprint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "from utils import vgg as nn\n",
    "from utils import roi_helpers\n",
    "from utils import losses as losses_fn\n",
    "from utils.simple_parser import get_data\n",
    "from utils import config, data_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 64\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigProto = tf.compat.v1.ConfigProto()\n",
    "ConfigProto.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=ConfigProto)\n",
    "# K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config has been written to config.pickle, and can be loaded when testing to ensure correct results\n"
     ]
    }
   ],
   "source": [
    "cfg = config.Config()\n",
    "\n",
    "# iteration 개수 (train: 1409, val: 456)\n",
    "cfg.num_epochs = 4 # 총 Epochs\n",
    "# cfg.rot_90 = True # default\n",
    "# cfg.use_random_brightness = True # add\n",
    "# cfg.use_vertical_flips = True # default\n",
    "# cfg.use_horizontal_flips = True # default\n",
    "cfg.num_rois = 128 # default\n",
    "cfg.std_scaling = 4 # default\n",
    "\n",
    "# TODO: the only file should to be change for other data to train\n",
    "cfg.model_path = './models/fusion_improved_last.hdf5'\n",
    "cfg.visual_model_path = './models/fusion_improved.png'\n",
    "cfg.train_label_file = './datav2/train_labels.txt' # txt 파일 경로 설정\n",
    "cfg.val_label_file = './datav2/val_labels.txt' # txt 파일 경로 설정\n",
    "cfg.class_mapping = {\n",
    "    'plane': 0, \n",
    "    'ship': 1, \n",
    "    'storage-tank': 2, \n",
    "    'baseball-diamond': 3, \n",
    "    'tennis-court': 4, \n",
    "    'basketball-court': 5, \n",
    "    'ground-track-field': 6, \n",
    "    'harbor': 7, \n",
    "    'bridge': 8, \n",
    "    'large-vehicle': 9, \n",
    "    'small-vehicle': 10, \n",
    "    'helicopter': 11, \n",
    "    'roundabout': 12, \n",
    "    'soccer-ball-field': 13, \n",
    "    'swimming-pool': 14, \n",
    "    'container-crane': 15, \n",
    "    'airport': 16, \n",
    "    'helipad': 17,\n",
    "    'bg': 18,\n",
    "}\n",
    "cfg.len_class = len(cfg.class_mapping)\n",
    "\n",
    "with open(cfg.config_save_file, 'wb') as config_f:\n",
    "    pickle.dump(cfg, config_f)\n",
    "    print('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(cfg.config_save_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load\n",
    "- train: 98990it (03:09)\n",
    "- valid: 28853it (01:02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "264038it [03:56, 1115.42it/s]\n",
      "80120it [01:14, 1079.29it/s]\n"
     ]
    }
   ],
   "source": [
    "train_images, train_classes_count = get_data(cfg.train_label_file, cfg.class_mapping, sep=',')\n",
    "validation_images, validation_classes_count = get_data(cfg.val_label_file, cfg.class_mapping, sep=',')\n",
    "\n",
    "with open('datav2/train_images.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(train_images, file, indent='\\t')\n",
    "    \n",
    "with open('datav2/train_classes_count.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(train_classes_count, file, indent='\\t')\n",
    "    \n",
    "with open('datav2/validation_images.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(validation_images, file, indent='\\t')\n",
    "    \n",
    "with open('datav2/validation_classes_count.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(validation_classes_count, file, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/dotav2_train_images_not_large.json', 'r', encoding='utf-8') as file:\n",
    "#     train_images = json.load(file)\n",
    "    \n",
    "# with open('data/dotav2_train_classes_count.json', 'r', encoding='utf-8') as file:\n",
    "#     train_classes_count = json.load(file)\n",
    "    \n",
    "# with open('data/dotav2_validation_images.json', 'r', encoding='utf-8') as file:\n",
    "#     validation_images = json.load(file)\n",
    "    \n",
    "# with open('data/dotav2_validation_classes_count.json', 'r', encoding='utf-8') as file:\n",
    "#     validation_classes_count = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes (including bg) = 19\n",
      "==Training Num samples 1824 , images per class:\n",
      "{'airport': 302,\n",
      " 'baseball-diamond': 428,\n",
      " 'basketball-court': 546,\n",
      " 'bg': 0,\n",
      " 'bridge': 2260,\n",
      " 'container-crane': 178,\n",
      " 'ground-track-field': 406,\n",
      " 'harbor': 6380,\n",
      " 'helicopter': 652,\n",
      " 'helipad': 8,\n",
      " 'large-vehicle': 24501,\n",
      " 'plane': 8171,\n",
      " 'roundabout': 589,\n",
      " 'ship': 38394,\n",
      " 'small-vehicle': 169268,\n",
      " 'soccer-ball-field': 361,\n",
      " 'storage-tank': 6802,\n",
      " 'swimming-pool': 2343,\n",
      " 'tennis-court': 2449}\n",
      "==Validation Num samples 592 , images per class:\n",
      "{'airport': 101,\n",
      " 'baseball-diamond': 227,\n",
      " 'basketball-court': 147,\n",
      " 'bg': 0,\n",
      " 'bridge': 494,\n",
      " 'container-crane': 14,\n",
      " 'ground-track-field': 166,\n",
      " 'harbor': 2249,\n",
      " 'helicopter': 78,\n",
      " 'helipad': 2,\n",
      " 'large-vehicle': 5371,\n",
      " 'plane': 2600,\n",
      " 'roundabout': 228,\n",
      " 'ship': 13466,\n",
      " 'small-vehicle': 50062,\n",
      " 'soccer-ball-field': 151,\n",
      " 'storage-tank': 3136,\n",
      " 'swimming-pool': 851,\n",
      " 'tennis-court': 777}\n"
     ]
    }
   ],
   "source": [
    "print('Num classes (including bg) = {}'.format(cfg.len_class))\n",
    "\n",
    "print('==Training Num samples {} , images per class:'.format(len(train_images)))\n",
    "pprint(train_classes_count)\n",
    "\n",
    "print('==Validation Num samples {} , images per class:'.format(len(validation_images)))\n",
    "pprint(validation_classes_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    data_gen_train = data_generators.get_anchor_gt(train_images, train_classes_count, cfg, nn.get_img_output_length, mode='train')\n",
    "    data_gen_val = data_generators.get_anchor_gt(validation_images, validation_classes_count, cfg, nn.get_img_output_length, mode='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    img_input = Input(shape=(None, None, 3), name='img_input')\n",
    "    roi_input = Input(shape=(None, 4), name='roi_input') # 4 == 점의 개수 * 2\n",
    "\n",
    "    # define the base network (resnet here, can be VGG, Inception, etc)\n",
    "    shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "    # define the RPN, built on the base layers\n",
    "    num_anchors = len(cfg.anchor_box_scales) * len(cfg.anchor_box_ratios) # 3 * 3 == 9\n",
    "    rpn = nn.rpn(shared_layers, num_anchors) # rpn_out_class 2k이어야 하는데 k 반환\n",
    "    \n",
    "    # cfg.num_rois : 32\n",
    "    classifier = nn.classifier(shared_layers, roi_input, cfg.num_rois, nb_classes=cfg.len_class, trainable=True)\n",
    "\n",
    "    model_rpn = Model(img_input, rpn[:2], name='model_rpn')\n",
    "    model_classifier = Model([img_input, roi_input], classifier, name='model_classifier')\n",
    "\n",
    "    # this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
    "    model_all = Model([img_input, roi_input], rpn[:2] + classifier, name='model_all')\n",
    "\n",
    "    model_rpn.compile(optimizer=Adam(lr=1e-5), \n",
    "                      loss=[losses_fn.rpn_loss_cls(num_anchors), losses_fn.rpn_loss_regr(num_anchors)])\n",
    "    \n",
    "    model_classifier.compile(optimizer=Adam(lr=1e-5),\n",
    "                             loss=[losses_fn.class_loss_cls, losses_fn.class_loss_regr(cfg.len_class - 1)],\n",
    "                             metrics={'dense_class_{}'.format(cfg.len_class): 'accuracy'})\n",
    "    \n",
    "    model_all.compile(optimizer='sgd', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model_all, to_file=cfg.visual_model_path, show_shapes=True, \\\n",
    "#            show_layer_names=True, expand_nested=False, dpi=200)\n",
    "\n",
    "# display(Image.open(cfg.visual_model_path))\n",
    "# model_all.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = 5# len(train_images)\n",
    "val_length = 5# len(validation_images)\n",
    "num_epochs = int(cfg.num_epochs)\n",
    "\n",
    "losses = np.zeros((train_length, 5))\n",
    "val_losses = np.zeros((val_length, 5))\n",
    "best_loss = np.Inf\n",
    "\n",
    "rpn_accuracy_for_epoch = []\n",
    "\n",
    "class_mapping_inv = {v: k for k, v in cfg.class_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\n",
      " P0215.png (1376, 3216, 3)\n",
      "#calc_rpn Exception : can't multiply sequence by non-int of type 'float'\n",
      "\n",
      " P1972.png (2592, 6640, 3)\n",
      "#calc_rpn Exception : can't multiply sequence by non-int of type 'float'\n",
      "\n",
      " P1827.png (1424, 1792, 3)\n",
      "\t rpn loss -> cls: 7.7625908851623535, regr: 1.8256162405014038\n",
      "\t cls loss -> cls: 2.9444386959075928, regr: 0.0, cls acc: 0.0\n",
      "0/5 [..............................] - ETA: 0s - rpn_cls: nan - rpn_regr: nan - detector_cls: nan - detector_regr: nan\n",
      " P0383.png (912, 1136, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3334: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t rpn loss -> cls: 7.493694305419922, regr: 1.7276943922042847\n",
      "\t cls loss -> cls: 2.9397969245910645, regr: 1.314517617225647, cls acc: 0.8984375\n",
      "1/5 [=====>........................] - ETA: 2:04:03 - rpn_cls: nan - rpn_regr: nan - detector_cls: nan - detector_regr: nan\n",
      " P3899.png (1024, 1024, 3)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x0000019BE4D66EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\t rpn loss -> cls: 7.97972297668457, regr: 1.9732075929641724\n",
      "\n",
      " P0892.png (2224, 2336, 3)\n",
      "#calc_rpn Exception : can't multiply sequence by non-int of type 'float'\n",
      "\n",
      " P0775.png (1888, 1792, 3)\n",
      "#calc_rpn Exception : can't multiply sequence by non-int of type 'float'\n",
      "\n",
      " P0109.png (1232, 1184, 3)\n",
      "\t rpn loss -> cls: 7.927600860595703, regr: 1.92723548412323\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_train_function.<locals>.train_function at 0x0000019BBE4B33A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\t cls loss -> cls: 2.9249818325042725, regr: 0.0, cls acc: 1.0\n",
      "3/5 [=================>............] - ETA: 45:27 - rpn_cls: nan - rpn_regr: nan - detector_cls: nan - detector_regr: nan  \n",
      " P2538.png (2224, 1888, 3)\n",
      "\t rpn loss -> cls: 7.803128242492676, regr: 1.7395098209381104\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 0.6\n",
      "Classifier accuracy for bounding boxes from RPN: 0.3796875\n",
      "Loss RPN classifier: 4.6367772102355955\n",
      "Loss RPN regression: 1.0961092233657836\n",
      "Loss Detector classifier: 1.761843490600586\n",
      "Loss Detector regression: 0.2629035234451294\n",
      "Elapsed time: 4171.743009567261\n",
      "\t Total loss decreasetd from inf to 7.75763, saving weights\n",
      "\n",
      " P2501.png (2464, 1792, 3)\n",
      "1/1 [==============================] - 0s 0s/step - loss: 9.6252 - rpn_out_class_loss: 7.7325 - rpn_out_regress_loss: 1.8928\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 4) for input Tensor(\"roi_input:0\", shape=(None, None, 4), dtype=float32), but it was called on an input with incompatible shape (None, 1, 0, 4).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 test_function  *\n        return step_function(self, iterator)\n    C:\\Users\\ipsl\\Desktop\\Faster-RCNN-for-Dota\\utils\\roi_pooling_conv.py:51 call  *\n        x = rois[0, roi_idx, 0]\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1013 _slice_helper\n        return strided_slice(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1186 strided_slice\n        op = gen_array_ops.strided_slice(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:10347 strided_slice\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:742 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:591 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3477 _create_op_internal\n        ret = Operation(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1974 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: slice index 0 of dimension 2 out of bounds. for '{{node model_classifier/roi_pooling_conv/strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=7](model_classifier/Cast, model_classifier/roi_pooling_conv/strided_slice/stack, model_classifier/roi_pooling_conv/strided_slice/stack_1, model_classifier/roi_pooling_conv/strided_slice/stack_2)' with input shapes: [?,1,0,4], [3], [3], [3] and with computed input tensors: input[1] = <0 0 0>, input[2] = <1 1 1>, input[3] = <1 1 1>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-84bcb4ac8051>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mselected_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mrpn_accuracy_for_epoch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[0mloss_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mY1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;31m#### loss 계산 갱신\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 test_function  *\n        return step_function(self, iterator)\n    C:\\Users\\ipsl\\Desktop\\Faster-RCNN-for-Dota\\utils\\roi_pooling_conv.py:51 call  *\n        x = rois[0, roi_idx, 0]\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1013 _slice_helper\n        return strided_slice(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1186 strided_slice\n        op = gen_array_ops.strided_slice(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:10347 strided_slice\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:742 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:591 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3477 _create_op_internal\n        ret = Operation(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1974 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: slice index 0 of dimension 2 out of bounds. for '{{node model_classifier/roi_pooling_conv/strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=7](model_classifier/Cast, model_classifier/roi_pooling_conv/strided_slice/stack, model_classifier/roi_pooling_conv/strided_slice/stack_1, model_classifier/roi_pooling_conv/strided_slice/stack_2)' with input shapes: [?,1,0,4], [3], [3], [3] and with computed input tensors: input[1] = <0 0 0>, input[2] = <1 1 1>, input[3] = <1 1 1>.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    for epoch_num in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n",
    "\n",
    "        start_time = time.time()\n",
    "        rpn_accuracy_for_epoch = []\n",
    "        progbar = Progbar(train_length)\n",
    "        \n",
    "        for iter_num in range(train_length):\n",
    "            X, Y, img_data, cache = next(data_gen_train)\n",
    "            \n",
    "            loss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "            print(f\"\\t rpn loss -> cls: {loss_rpn[1]}, regr: {loss_rpn[2]}\")\n",
    "            P_rpn = model_rpn.predict_on_batch(X) # (rpn cls, rpn regr)\n",
    "            result = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], cfg, 'tf', use_regr=True,\n",
    "                                            overlap_thresh=0.7,\n",
    "                                            max_boxes=256) # origin code is 300 but paper is set by 256 (128 - positive, 128 - negative)\n",
    "\n",
    "            # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "            X2, Y1, Y2, IouS = roi_helpers.calc_iou(result, img_data, cfg, cfg.class_mapping)\n",
    "            \n",
    "            if X2 is None:\n",
    "                rpn_accuracy_for_epoch.append(0)\n",
    "                continue\n",
    "\n",
    "            pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "            neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "            rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "            \n",
    "            if len(neg_samples) > 0:\n",
    "                neg_samples = neg_samples[0]\n",
    "            else:\n",
    "                neg_samples = []\n",
    "\n",
    "            if len(pos_samples) > 0:\n",
    "                pos_samples = pos_samples[0]\n",
    "            else:\n",
    "                pos_samples = []\n",
    "            \n",
    "            if len(pos_samples) < cfg.num_rois // 2:\n",
    "                selected_pos_samples = pos_samples.tolist()\n",
    "            else:\n",
    "                selected_pos_samples = np.random.choice(pos_samples, cfg.num_rois // 2, replace=False).tolist()\n",
    "\n",
    "            if len(neg_samples) + len(selected_pos_samples) > cfg.num_rois:\n",
    "                selected_neg_samples = np.random.choice(neg_samples, cfg.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "            else:\n",
    "                selected_neg_samples = np.random.choice(neg_samples, cfg.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "            selected_samples = selected_pos_samples + selected_neg_samples\n",
    "            \n",
    "            loss_class = model_classifier.train_on_batch([X, X2[:, selected_samples, :]], [Y1[:, selected_samples, :], Y2[:, selected_samples, :]])\n",
    "            print(f\"\\t cls loss -> cls: {loss_class[1]}, regr: {loss_class[2]}, cls acc: {loss_class[3]}\")\n",
    "            \n",
    "            #### loss 계산 갱신\n",
    "            \n",
    "            losses[iter_num, 0] = loss_rpn[1]\n",
    "            losses[iter_num, 1] = loss_rpn[2]\n",
    "\n",
    "            losses[iter_num, 2] = loss_class[1]\n",
    "            losses[iter_num, 3] = loss_class[2]\n",
    "            losses[iter_num, 4] = loss_class[3]\n",
    "            \n",
    "            rpn_cls = np.mean(losses[:iter_num, 0])\n",
    "            rpn_regr = np.mean(losses[:iter_num, 1])\n",
    "            detector_cls = np.mean(losses[:iter_num, 2])\n",
    "            detector_regr = np.mean(losses[:iter_num, 3])\n",
    "\n",
    "            progbar.update(iter_num,\n",
    "                           [('rpn_cls', rpn_cls), \\\n",
    "                            ('rpn_regr', rpn_regr), \\\n",
    "                            ('detector_cls', detector_cls), \\\n",
    "                            ('detector_regr', detector_regr)])\n",
    "            if iter_num % 300 == 299:\n",
    "                model_all.save_weights(os.path.join('models', f'Epoch_{epoch_num}_Iter_{iter_num}_rpnCls_{rpn_cls:.4f}_rpnRegr_{rpn_regr:.4f}_clsCls_{detector_cls:.4f}_clsRegr_{detector_regr:.4f}.hdf5'))\n",
    "\n",
    "        loss_rpn_cls = np.mean(losses[:, 0])\n",
    "        loss_rpn_regr = np.mean(losses[:, 1])\n",
    "        loss_class_cls = np.mean(losses[:, 2])\n",
    "        loss_class_regr = np.mean(losses[:, 3])\n",
    "        class_acc = np.mean(losses[:, 4])\n",
    "\n",
    "        mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "\n",
    "        if cfg.verbose:\n",
    "            print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "            if mean_overlapping_bboxes == 0:\n",
    "                print('RPN is not producing bounding boxes that overlap the ground truth boxes.')\n",
    "                print('Check RPN settings or keep training.')\n",
    "                \n",
    "            print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "            print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "            print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "            print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "            print('Loss Detector regression: {}'.format(loss_class_regr))\n",
    "            print('Elapsed time: {}'.format(time.time() - start_time))\n",
    "\n",
    "        curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "\n",
    "        if curr_loss < best_loss:\n",
    "            print(f'\\t Total loss decreasetd from {best_loss:.5f} to {curr_loss:.5f}, saving weights')\n",
    "            best_loss = curr_loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Validation Check\n",
    "        \n",
    "        start_time = time.time()\n",
    "        rpn_accuracy_for_epoch = []\n",
    "        progbar = Progbar(val_length)\n",
    "                    \n",
    "        for iter_num in range(val_length):\n",
    "            X, Y, img_data, cache = next(data_gen_train)\n",
    "            \n",
    "            loss_rpn = model_rpn.evaluate(X, Y)\n",
    "            P_rpn = model_rpn.predict_on_batch(X)\n",
    "            result = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], cfg, 'tf', use_regr=True,\n",
    "                                            overlap_thresh=0.7,\n",
    "                                            max_boxes=256) # origin code is 300 but paper is set by 256 (128 - positive, 128 - negative)\n",
    "\n",
    "            # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "            X2, Y1, Y2, IouS = roi_helpers.calc_iou(result, img_data, cfg, cfg.class_mapping)\n",
    "            \n",
    "            if X2 is None:\n",
    "                rpn_accuracy_for_epoch.append(0)\n",
    "                continue\n",
    "\n",
    "            selected_samples = np.where(Y1[0, :, -1] == 0)\n",
    "            rpn_accuracy_for_epoch.append((len(selected_samples)))\n",
    "            loss_class = model_classifier.evaluate([X, X2[:, selected_samples, :]], [Y1[:, selected_samples, :], Y2[:, selected_samples, :]])\n",
    "            \n",
    "            #### loss 계산 갱신\n",
    "            \n",
    "            val_losses[iter_num, 0] = loss_rpn[1]\n",
    "            val_losses[iter_num, 1] = loss_rpn[2]\n",
    "\n",
    "            val_losses[iter_num, 2] = loss_class[1]\n",
    "            val_losses[iter_num, 3] = loss_class[2]\n",
    "            val_losses[iter_num, 4] = loss_class[3]\n",
    "            \n",
    "            rpn_cls = np.nanmean(val_losses[:iter_num, 0])\n",
    "            rpn_regr = np.nanmean(val_losses[:iter_num, 1])\n",
    "            detector_cls = np.nanmean(val_losses[:iter_num, 2])\n",
    "            detector_regr = np.nanmean(val_losses[:iter_num, 3])\n",
    "\n",
    "            progbar.update(iter_num,\n",
    "                           [('rpn_cls', rpn_cls), \\\n",
    "                            ('rpn_regr', rpn_regr), \\\n",
    "                            ('detector_cls', detector_cls), \\\n",
    "                            ('detector_regr', detector_regr)])        \n",
    "        \n",
    "        \n",
    "        loss_rpn_cls = np.mean(val_losses[:, 0])\n",
    "        loss_rpn_regr = np.mean(val_losses[:, 1])\n",
    "        loss_class_cls = np.mean(val_losses[:, 2])\n",
    "        loss_class_regr = np.mean(val_losses[:, 3])\n",
    "        class_acc = np.mean(val_losses[:, 4])\n",
    "\n",
    "        mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "\n",
    "        print('Val Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "        if mean_overlapping_bboxes == 0:\n",
    "            print('Val RPN is not producing bounding boxes that overlap the ground truth boxes.')\n",
    "            print('Val Check RPN settings or keep training.')\n",
    "\n",
    "        print('Val Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "        print('Val Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "        print('Val Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "        print('Val Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "        print('Val Loss Detector regression: {}'.format(loss_class_regr))\n",
    "        print('Val Elapsed time: {}'.format(time.time() - start_time))\n",
    "        \n",
    "        model_all.save_weights(os.path.join('models', f'Epoch_{epoch_num}_rpnCls_{loss_rpn_cls:.4f}_rpnRegr_{loss_rpn_regr:.4f}_clsCls_{loss_class_cls:.4f}_clsRegr_{loss_class_regr:.4f}.hdf5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([12,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
